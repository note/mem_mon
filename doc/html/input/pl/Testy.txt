Title: Testy

---

##Domyślne działanie alokatora

Program dokonuje 300 alokacji po 4 kB. Fragment wyjścia `memtrace`:

~~~
Process is trying to allocate 136KB
Process has allocated 136KB
Process is trying to allocate 132KB
Process has allocated 132KB
Process is trying to allocate 132KB
Process has allocated 132KB
.
.
.
Process is trying to allocate 132KB
Process has allocated 132KB
~~~

Przy 300 alokacjach po 40 kB otrzymujemy inny wynik. Jak widać alokator reaguje na taką zmianę i w pojedynczym wywołaniu rezerwuje więcej pamięci.

~~~
Process is trying to allocate 172KB
Process has allocated 172KB
Process is trying to allocate 160KB
Process has allocated 160KB
Process is trying to allocate 160KB
Process has allocated 160KB
.
.
.
Process is trying to allocate 160KB
Process has allocated 160KB
~~~

Przy zwiększeniu pojedynczej alokacji do 400kB otrzymujemy zupełnie inny wynik:

~~~
Process is trying to create private mapping of size 404KB (anonymous)
Process has created private mapping of size 404KB (anonymous)
Process is trying to create private mapping of size 404KB (anonymous)
Process has created private mapping of size 404KB (anonymous)
.
.
.
Process is trying to create private mapping of size 404KB (anonymous)
Process has created private mapping of size 404KB (anonymous)
~~~

Malloc dla odpowiednio dużych żądań nie korzysta z wywołania brk, ale z mapowania pamięci mmap. Próg, od którego malloc zaczyna mapować można ustawić korzystając z **mallopt**. Można całkowicie zrezygnować z mapowania wywołując `mallopt(M_MMAP_MAX, 0)`:

~~~
Process is trying to allocate 532KB
Process has allocated 532KB
Process is trying to allocate 400KB
Process has allocated 400KB
Process is trying to allocate 400KB
Process has allocated 400KB
.
.
.
Process is trying to allocate 400KB
Process has allocated 400KB
~~~

##Konfiguracja alokatora przy pomocy mallopt

Istotne parametry mallopt:

+ M_MXFAST - wolne kawałki, które są większe niż ta wartość są łączone.
+ M_TRIM_THRESHOLD - nieużywana pamięć jest zwracana do systemu operacyjnego, gdy jej wielkość przekracza tę wartość. Nie zawsze zwrócenie pamięci jest możliwe, jest tak np. jeśli na szczycie segmentu danych znajduje się używany blok. Małe wartości spowodują czestsze zwracanie pamięci do systemu co zaowocuje mniejszym zużyciem pamięci, jednak obarczone to będzie koniecznością wykonania większej ilości wywołań systemowych.
+ M_TOP_PAD - podana wartość określa wielkość pamięci, która będzie zaalokowana w dodatku do pamięci, która jest niezbędna do wykonania żądania
+M_MMAP_THRESHOLD - próg od którego malloc zaczyna korzystać z mmap() zamiast z brk()

##Wyniki testów

Przetestowałem 4 konfiguracje alokatora.

+ fast, nastawiona na prędkość działania

~~~
mallopt(M_TRIM_THRESHOLD, -1);
~~~

+ fastest, nastawiona wyłącznie na prędkość działania

~~~
mallopt(M_TRIM_THRESHOLD, -1);
mallopt(M_TOP_PAD, 4096*1000);
~~~

+ small, nastawiona na małe zużycie pamięci

~~~
mallopt(M_TRIM_THRESHOLD, 100);
~~~

+ smallest, nastawiona wyłącznie na małe zużycie pamięci

~~~
mallopt(M_TRIM_THRESHOLD, 1024*40);
~~~


Dla programu `perf.cpp` (wszystkie pliki testowe znajdują się w katalogu `tests`):

Konfiguracja | Czas | brk rozszerzających | brk zmniejszających | virtual memory size
- | - | - | - | -
fast|  0.324s | 3043 | 0 | 396MB
fastest  | 0.312s | 102 | 0 | 399MB
small | 0.466s | 5998 | 97562 | 4400KB
smallest | 0.472s | 5988 | 97562 | 4004KB

##Planowane testy

Niestety nie zdążyłem przeprowadzić tylu testów ile chciałem. Spowodowane jest to faktem, że skupiłem się głównie na części implementacyjnej zadania. Jak widać po wynikach prostego testu, który przeprowadziłem konfiguracja alokatora ma bardzo duże znaczenie dla wydajności aplikacji i warto by zbadać dokładniej tą tematyke oraz spróbować zauważyc pewne regularności, które pomogłyby w konfigurowaniu alokatora pod konkretne zastosowanie. Chciałem również przetestować prawdziwe aplikacje. Nie wykluczam również napisania własnego alokatora, aby mieć całkowitą kontrolę nad alokacją i zwracaniem pamięci do systemu.

##Środowisko
Wszystkie testy przeprowadzono na Linuxie z jądrem 2.6.34-gentoo-r1, Intel(R) Core(TM)2 Duo CPU T9600 @ 2.80GHz.